{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges encountered\n",
    "\n",
    "1. Open-source models were not usable due to their large sizes.\n",
    "2. Hit the limit on HFhub, resulting in errors.\n",
    "3. Fine-tuning was not completed due to the large model size and limited PC specifications.\n",
    "\n",
    "# Sections\n",
    "\n",
    "1. Main includes final code\n",
    "2. Rough/Experiments: provides a detailed, step-by-step process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "This section includes the final code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Docs: 2\n",
      "Preprocessing corpus... \n",
      "The categories covered by the paper \"TextGrad: Automatic Differentiation via Text\" include Question Answering and Specificity.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "class QAsystem:\n",
    "    def __init__(self, pdf_directory, openai_api_key):\n",
    "        self.pdf_directory = pdf_directory\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.load_pdf_documents()\n",
    "        self.clean_corpus()\n",
    "        self.create_text_splitter()\n",
    "        self.create_openai_client()\n",
    "        self.create_retriever()\n",
    "        self.create_contextualize_question_prompt()\n",
    "        self.create_qa_prompt()\n",
    "        self.create_rag_chain()\n",
    "        self.create_conversational_rag_chain()\n",
    "\n",
    "    def load_pdf_documents(self):\n",
    "        try:\n",
    "            self.loader = PyPDFDirectoryLoader(self.pdf_directory)\n",
    "            self.pages = self.loader.load()\n",
    "            print(f\"Total Docs: {len(self.pages)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading PDF: {e}\")\n",
    "\n",
    "    def clean_corpus(self):\n",
    "        self.corpus = \" \".join(\n",
    "            [page.page_content.replace(\"\\t\", \" \") for page in self.pages]\n",
    "        )\n",
    "\n",
    "        \"\"\"Cleaning using RegEx\"\"\"\n",
    "        print(\"Preprocessing corpus... \")\n",
    "        self.cleaned_corpus = self.corpus\n",
    "\n",
    "    def create_text_splitter(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=10,\n",
    "            length_function=len,\n",
    "        )\n",
    "        splitted_corpus = self.text_splitter.split_text(self.cleaned_corpus)\n",
    "        self.splitted_corpus_in_docs = self.text_splitter.create_documents(\n",
    "            splitted_corpus\n",
    "        )\n",
    "\n",
    "    def create_openai_client(self):\n",
    "        self.openAIclient = ChatOpenAI(\n",
    "            api_key=self.openai_api_key,\n",
    "            # callbacks=[ContextCallbackHandler(token=\"C2nN2SuVyaKE92pGT3HtcSsY\")],\n",
    "        )\n",
    "\n",
    "    def create_retriever(self):\n",
    "        self.openaiEmbeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key)\n",
    "        self.vectordb = FAISS.from_documents(\n",
    "            self.splitted_corpus_in_docs, self.openaiEmbeddings\n",
    "        )\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "    def create_contextualize_question_prompt(self):\n",
    "        self.contextualize_q_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "        self.contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.history_aware_retriever = create_history_aware_retriever(\n",
    "            self.openAIclient, self.retriever, self.contextualize_q_prompt\n",
    "        )\n",
    "\n",
    "    def create_qa_prompt(self):\n",
    "        self.system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know. Use three sentences maximum and keep the \"\n",
    "            \"answer concise.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "        self.qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.question_answer_chain = create_stuff_documents_chain(\n",
    "            self.openAIclient, self.qa_prompt\n",
    "        )\n",
    "\n",
    "    def create_rag_chain(self):\n",
    "        self.rag_chain = create_retrieval_chain(\n",
    "            self.history_aware_retriever, self.question_answer_chain\n",
    "        )\n",
    "\n",
    "    def create_conversational_rag_chain(self):\n",
    "        self.store = {}\n",
    "\n",
    "        def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "            if session_id not in self.store:\n",
    "                self.store[session_id] = ChatMessageHistory()\n",
    "            return self.store[session_id]\n",
    "\n",
    "        self.conversational_rag_chain = RunnableWithMessageHistory(\n",
    "            self.rag_chain,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            output_messages_key=\"answer\",\n",
    "        )\n",
    "\n",
    "    def ask_question(self, question, session_id=\"abc123\"):\n",
    "        response = self.conversational_rag_chain.invoke(\n",
    "            {\"input\": question},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )[\"answer\"]\n",
    "        return response\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAIAPIKEY\")\n",
    "if not openai_api_key:\n",
    "    # print(f\"OpenAI API Key: {openai_api_key}\")\n",
    "    # else:\n",
    "    print(\"OPENAIAPIKEY environment variable not set.\")\n",
    "\n",
    "pdf_qa_system = QAsystem(pdf_directory=\"pdfs\", openai_api_key=openai_api_key)\n",
    "response = pdf_qa_system.ask_question(\n",
    "    \"List the categories covered by the paper titled 'TextGrad: Automatic Differentiation viaText'.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough/Experiments\n",
    "\n",
    "This section provides a detailed, step-by-step process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_openai\n",
    "# !pip install langchain_core\n",
    "# !pip install python-dotenv\n",
    "# !pip install langchain_community\n",
    "# !pip install pypdf\n",
    "# !pip install langchainhub\n",
    "\n",
    "\n",
    "\n",
    "! pip install langchain-community\n",
    "! pip install pypdf\n",
    "! pip install langchain-openai\n",
    "! pip install python-dotenv\n",
    "# ! pip install sentence_transformers\n",
    "! pip install langchain_huggingface\n",
    "! pip install ipywidgets\n",
    "! pip install faiss-cpu\n",
    "! pip install --upgrade context-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Docs: 2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "try:\n",
    "    loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "    pages = loader.load()\n",
    "    print(f\"Total Docs: {len(pages)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Title:  MeshAnything: Artist -Created Mesh Generation with Autoregressive Transformers  \n",
      "Authors:  buaacyw/meshanything  \n",
      "Date:  14 Jun 2024  \n",
      "Description:  Recently, 3D assets created via reconstruction and generation have matched the \n",
      "quality of manually crafted assets, highlighting their potential for replacement.  \n",
      "Stats:  417, 5.09 stars / hour  \n",
      "Categories:  Decoder  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  Accessing GPT -4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self -\n",
      "refine with LLaMa -3 8B  \n",
      "Authors:  trotsky1997/mathblackbox  \n",
      "Date:  11 Jun 2024  \n",
      "Description:  This paper introduces the MCT Self -Refine algorithm, an innovative integration of \n",
      "Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance \n",
      "performance in complex mathematical reasoning tasks.  \n",
      "Stats:  279, 2.35 stars / hour  \n",
      "Categories:  Decision Making, GSM8K +2  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  TextGrad: Automatic 'Differentiation' via Text  \n",
      "Authors:  zou-group/textgrad  \n",
      "Date:  11 Jun 2024  \n",
      "Description:  Without modifying the framework, TextGrad improves the zero -shot accuracy of \n",
      "GPT -4o in Google -Proof Question Answering, yields significant relative performance gain in \n",
      "optimizing LeetCode -Hard coding problem solutions, improves prompts for reasoning, desi gns \n",
      "new druglike small molecules with desirable in silico binding, and designs radiation oncology \n",
      "treatment plans with high specificity.  \n",
      "Stats:  485, 2.04 stars / hour  \n",
      "Categories:  Question Answering, Specificity  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  Scalable MatMul -free Language Modeling  \n",
      "Authors:  ridgerchu/matmulfreellm  \n",
      "Date:  4 Jun 2024  \n",
      "Description:  Our experiments show that our proposed MatMul -free models achieve performance \n",
      "on-par with state -of-the-art Transformers that require far more memory during inference at a \n",
      "scale up to at least 2.7B parameters.  \n",
      "Stats:  2,140, 1.98 stars / hour  ' metadata={'source': 'pdfs/RAG Input Doc.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Categories:  Language Modelling  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  VideoLLaMA 2: Advancing Spatial -Temporal Modeling and Audio Understanding in \n",
      "Video -LLMs  \n",
      "Authors:  damo -nlp-sg/videollama2  \n",
      "Date:  11 Jun 2024  \n",
      "Description:  In this paper, we present the VideoLLaMA 2, a set of Video Large Language \n",
      "Models (Video -LLMs) designed to enhance spatial -temporal modeling and audio understanding \n",
      "in video and audio -oriented tasks.  \n",
      "Stats:  318, 1.50 stars / hour  \n",
      "Categories:  Multiple -choice, Question Answering +3  \n",
      "Links:  Paper, Code  \n",
      " ' metadata={'source': 'pdfs/RAG Input Doc.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "print(pages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of Corpus: 2462, \n",
      "\n",
      "\n",
      "corpus[:100]: Title:  MeshAnything: Artist -Created Mesh Generation with Autoregressive Transformers  \n",
      "Authors:  b\n"
     ]
    }
   ],
   "source": [
    "content = pages\n",
    "corpus = \" \".join([page.page_content.replace(\"\\t\", \" \") for page in content])\n",
    "print(f\"length of Corpus: {len(corpus)}, \\n\\n\\ncorpus[:100]: {corpus[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  MeshAnything: Artist -Created Mesh Generation with Autoregressive Transformers  \n",
      "Authors:  buaacyw/meshanything  \n",
      "Date:  14 Jun 2024  \n",
      "Description:  Recently, 3D assets created via reconstruction and generation have matched the \n",
      "quality of manually crafted assets, highlighting their potential for replacement.  \n",
      "Stats:  417, 5.09 stars / hour  \n",
      "Categories:  Decoder  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  Accessing GPT -4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self -\n",
      "refine with LLaMa -3 8B  \n",
      "Authors:  trotsky1997/mathblackbox  \n",
      "Date:  11 Jun 2024  \n",
      "Description:  This paper introduces the MCT Self -Refine algorithm, an innovative integration of \n",
      "Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance \n",
      "performance in complex mathematical reasoning tasks.  \n",
      "Stats:  279, 2.35 stars / hour  \n",
      "Categories:  Decision Making, GSM8K +2  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  TextGrad: Automatic 'Differentiation' via Text  \n",
      "Authors:  zou-group/textgrad  \n",
      "Date:  11 Jun 2024  \n",
      "Description:  Without modifying the framework, TextGrad improves the zero -shot accuracy of \n",
      "GPT -4o in Google -Proof Question Answering, yields significant relative performance gain in \n",
      "optimizing LeetCode -Hard coding problem solutions, improves prompts for reasoning, desi gns \n",
      "new druglike small molecules with desirable in silico binding, and designs radiation oncology \n",
      "treatment plans with high specificity.  \n",
      "Stats:  485, 2.04 stars / hour  \n",
      "Categories:  Question Answering, Specificity  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  Scalable MatMul -free Language Modeling  \n",
      "Authors:  ridgerchu/matmulfreellm  \n",
      "Date:  4 Jun 2024  \n",
      "Description:  Our experiments show that our proposed MatMul -free models achieve performance \n",
      "on-par with state -of-the-art Transformers that require far more memory during inference at a \n",
      "scale up to at least 2.7B parameters.  \n",
      "Stats:  2,140, 1.98 stars / hour   Categories:  Language Modelling  \n",
      "Links:  Paper, Code  \n",
      " \n",
      "Title:  VideoLLaMA 2: Advancing Spatial -Temporal Modeling and Audio Understanding in \n",
      "Video -LLMs  \n",
      "Authors:  damo -nlp-sg/videollama2  \n",
      "Date:  11 Jun 2024  \n",
      "Description:  In this paper, we present the VideoLLaMA 2, a set of Video Large Language \n",
      "Models (Video -LLMs) designed to enhance spatial -temporal modeling and audio understanding \n",
      "in video and audio -oriented tasks.  \n",
      "Stats:  318, 1.50 stars / hour  \n",
      "Categories:  Multiple -choice, Question Answering +3  \n",
      "Links:  Paper, Code  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_corpus(text):\n",
    "    \"\"\"Nothing to clean yet\"\"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "cleaned_corpus = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable\n",
    "openai_api_key = os.getenv(\"OPENAIAPIKEY\")\n",
    "# Use the environment variable\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key: {openai_api_key}\")\n",
    "else:\n",
    "    print(\"OPENAIAPIKEY environment variable not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks.context_callback import ContextCallbackHandler\n",
    "\n",
    "openAIclient = ChatOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    # model_name = \"gpt-3.5-turbo-16k\", #default4k\n",
    "    # temperature=0.1,\n",
    "    callbacks=[ContextCallbackHandler(token=\"C2nN2SuVyaKE92pGT3HtcSsY\")],\n",
    ")\n",
    "\n",
    "openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get number of clients\n",
    "# print(f\"total number of tokens in token: {openAIclient.get_num_tokens(cleaned_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,  # gpt-3-5-turbo 179) , the docs quote a 16k context window\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "\n",
    "splitted_corpus = text_splitter.split_text(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,\n",
       " 'Title:  MeshAnything: Artist -Created Mesh Generation with Autoregressive Transformers')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_corpus = text_splitter.split_text(cleaned_corpus)\n",
    "len(splitted_corpus), splitted_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Title:  MeshAnything: Artist -Created Mesh Generation with Autoregressive Transformers'),\n",
       " Document(page_content='Authors:  buaacyw/meshanything  \\nDate:  14 Jun 2024'),\n",
       " Document(page_content='Description:  Recently, 3D assets created via reconstruction and generation have matched the'),\n",
       " Document(page_content='quality of manually crafted assets, highlighting their potential for replacement.'),\n",
       " Document(page_content='Stats:  417, 5.09 stars / hour  \\nCategories:  Decoder  \\nLinks:  Paper, Code'),\n",
       " Document(page_content='Title:  Accessing GPT -4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self -'),\n",
       " Document(page_content='refine with LLaMa -3 8B  \\nAuthors:  trotsky1997/mathblackbox  \\nDate:  11 Jun 2024'),\n",
       " Document(page_content='Description:  This paper introduces the MCT Self -Refine algorithm, an innovative integration of'),\n",
       " Document(page_content='Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance'),\n",
       " Document(page_content='performance in complex mathematical reasoning tasks.  \\nStats:  279, 2.35 stars / hour'),\n",
       " Document(page_content='Categories:  Decision Making, GSM8K +2  \\nLinks:  Paper, Code'),\n",
       " Document(page_content=\"Title:  TextGrad: Automatic 'Differentiation' via Text  \\nAuthors:  zou-group/textgrad\"),\n",
       " Document(page_content='Date:  11 Jun 2024'),\n",
       " Document(page_content='Description:  Without modifying the framework, TextGrad improves the zero -shot accuracy of'),\n",
       " Document(page_content='GPT -4o in Google -Proof Question Answering, yields significant relative performance gain in'),\n",
       " Document(page_content='optimizing LeetCode -Hard coding problem solutions, improves prompts for reasoning, desi gns'),\n",
       " Document(page_content='new druglike small molecules with desirable in silico binding, and designs radiation oncology'),\n",
       " Document(page_content='treatment plans with high specificity.  \\nStats:  485, 2.04 stars / hour'),\n",
       " Document(page_content='Categories:  Question Answering, Specificity  \\nLinks:  Paper, Code'),\n",
       " Document(page_content='Title:  Scalable MatMul -free Language Modeling  \\nAuthors:  ridgerchu/matmulfreellm'),\n",
       " Document(page_content='Date:  4 Jun 2024'),\n",
       " Document(page_content='Description:  Our experiments show that our proposed MatMul -free models achieve performance'),\n",
       " Document(page_content='on-par with state -of-the-art Transformers that require far more memory during inference at a'),\n",
       " Document(page_content='scale up to at least 2.7B parameters.'),\n",
       " Document(page_content='Stats:  2,140, 1.98 stars / hour   Categories:  Language Modelling  \\nLinks:  Paper, Code'),\n",
       " Document(page_content='Title:  VideoLLaMA 2: Advancing Spatial -Temporal Modeling and Audio Understanding in'),\n",
       " Document(page_content='Video -LLMs  \\nAuthors:  damo -nlp-sg/videollama2  \\nDate:  11 Jun 2024'),\n",
       " Document(page_content='Description:  In this paper, we present the VideoLLaMA 2, a set of Video Large Language'),\n",
       " Document(page_content='Models (Video -LLMs) designed to enhance spatial -temporal modeling and audio understanding'),\n",
       " Document(page_content='in video and audio -oriented tasks.  \\nStats:  318, 1.50 stars / hour'),\n",
       " Document(page_content='Categories:  Multiple -choice, Question Answering +3  \\nLinks:  Paper, Code')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_corpus_in_docs = text_splitter.create_documents(splitted_corpus)\n",
    "splitted_corpus_in_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_TOKEN='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/lib/python3.11/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 51, in main\n",
      "    service.run()\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/lib/python3.11/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/lib/python3.11/site-packages/huggingface_hub/_login.py\", line 115, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/lib/python3.11/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/lib/python3.11/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zohaib/anaconda3/envs/rag_langchain/lib/python3.11/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 319, in decode\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# ! huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mzohaibnasir\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Load the model\n",
    "# embeddingsModel = SentenceTransformer(\"Linq-AI-Research/Linq-Embed-Mistral\")\n",
    "# embeddingsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# embeddingsModel = HuggingFaceEmbeddings(model_name=\"Alibaba-NLP/gte-Qwen2-7B-instruct\")\n",
    "\n",
    "\n",
    "embeddingsModel = HuggingFaceEmbeddings(\n",
    "    model_name=\"Linq-AI-Research/Linq-Embed-Mistral\"\n",
    ")\n",
    "embeddingsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='llama2', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollamaEmbeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")  # by default, uses llama2. Run `ollama pull llama2` to pull down the model\n",
    "ollamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "openaiEmbeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "openaiEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(client=<InferenceClient(model='Alibaba-NLP/gte-Qwen2-7B-instruct', timeout=None)>, repo_id='Alibaba-NLP/gte-Qwen2-7B-instruct', task='text2text-generation')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "hf = HuggingFaceHub(\n",
    "    # repo_id=\"Linq-AI-Research/Linq-Embed-Mistral\",\n",
    "    repo_id=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "    task=\"text2text-generation\",\n",
    "    #  model_kwargs={\n",
    "    #     \"max_new_tokens\": 512,\n",
    "    #     \"top_k\": 30,\n",
    "    #     \"temperature\": 0.1,\n",
    "    #     \"repetition_penalty\": 1.03,\n",
    "    # },\n",
    ")\n",
    "\n",
    "\n",
    "hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/zohaib/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='Alibaba-NLP/gte-Qwen2-7B-instruct', model='Alibaba-NLP/gte-Qwen2-7B-instruct', client=<InferenceClient(model='Alibaba-NLP/gte-Qwen2-7B-instruct', timeout=120)>, async_client=<InferenceClient(model='Alibaba-NLP/gte-Qwen2-7B-instruct', timeout=120)>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    # repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    repo_id=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "    # max_length=128,\n",
    "    # temperature=0.5,\n",
    "    # huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Title:  MeshAnything: Artist -Created Mesh Generation with Autoregressive Transformers'),\n",
       " Document(page_content='Authors:  buaacyw/meshanything  \\nDate:  14 Jun 2024'),\n",
       " Document(page_content='Description:  Recently, 3D assets created via reconstruction and generation have matched the'),\n",
       " Document(page_content='quality of manually crafted assets, highlighting their potential for replacement.'),\n",
       " Document(page_content='Stats:  417, 5.09 stars / hour  \\nCategories:  Decoder  \\nLinks:  Paper, Code'),\n",
       " Document(page_content='Title:  Accessing GPT -4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self -'),\n",
       " Document(page_content='refine with LLaMa -3 8B  \\nAuthors:  trotsky1997/mathblackbox  \\nDate:  11 Jun 2024'),\n",
       " Document(page_content='Description:  This paper introduces the MCT Self -Refine algorithm, an innovative integration of'),\n",
       " Document(page_content='Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance'),\n",
       " Document(page_content='performance in complex mathematical reasoning tasks.  \\nStats:  279, 2.35 stars / hour'),\n",
       " Document(page_content='Categories:  Decision Making, GSM8K +2  \\nLinks:  Paper, Code'),\n",
       " Document(page_content=\"Title:  TextGrad: Automatic 'Differentiation' via Text  \\nAuthors:  zou-group/textgrad\"),\n",
       " Document(page_content='Date:  11 Jun 2024'),\n",
       " Document(page_content='Description:  Without modifying the framework, TextGrad improves the zero -shot accuracy of'),\n",
       " Document(page_content='GPT -4o in Google -Proof Question Answering, yields significant relative performance gain in'),\n",
       " Document(page_content='optimizing LeetCode -Hard coding problem solutions, improves prompts for reasoning, desi gns'),\n",
       " Document(page_content='new druglike small molecules with desirable in silico binding, and designs radiation oncology'),\n",
       " Document(page_content='treatment plans with high specificity.  \\nStats:  485, 2.04 stars / hour'),\n",
       " Document(page_content='Categories:  Question Answering, Specificity  \\nLinks:  Paper, Code'),\n",
       " Document(page_content='Title:  Scalable MatMul -free Language Modeling  \\nAuthors:  ridgerchu/matmulfreellm'),\n",
       " Document(page_content='Date:  4 Jun 2024'),\n",
       " Document(page_content='Description:  Our experiments show that our proposed MatMul -free models achieve performance'),\n",
       " Document(page_content='on-par with state -of-the-art Transformers that require far more memory during inference at a'),\n",
       " Document(page_content='scale up to at least 2.7B parameters.'),\n",
       " Document(page_content='Stats:  2,140, 1.98 stars / hour   Categories:  Language Modelling  \\nLinks:  Paper, Code'),\n",
       " Document(page_content='Title:  VideoLLaMA 2: Advancing Spatial -Temporal Modeling and Audio Understanding in'),\n",
       " Document(page_content='Video -LLMs  \\nAuthors:  damo -nlp-sg/videollama2  \\nDate:  11 Jun 2024'),\n",
       " Document(page_content='Description:  In this paper, we present the VideoLLaMA 2, a set of Video Large Language'),\n",
       " Document(page_content='Models (Video -LLMs) designed to enhance spatial -temporal modeling and audio understanding'),\n",
       " Document(page_content='in video and audio -oriented tasks.  \\nStats:  318, 1.50 stars / hour'),\n",
       " Document(page_content='Categories:  Multiple -choice, Question Answering +3  \\nLinks:  Paper, Code')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_corpus_in_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ae568b8f190>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vectordb = FAISS.from_documents(splitted_corpus_in_docs, openaiEmbeddings)\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"Scalable MatMul-free Language Modeling\" by ridgerchu/matmulfreellm deals with language modeling and its scalability.\n"
     ]
    }
   ],
   "source": [
    "# # Load the llm\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# # Define prompt template\n",
    "# template = \"\"\"\n",
    "# You are an assistant for question-answering tasks.\n",
    "# Use the provided context only to answer the following question:\n",
    "\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "\n",
    "# Question: {input}\n",
    "# \"\"\"\n",
    "\n",
    "# # Create a prompt template\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# # Create a chain\n",
    "# doc_chain = create_stuff_documents_chain(openAIclient, prompt)\n",
    "# chain = create_retrieval_chain(retriever, doc_chain)\n",
    "\n",
    "\n",
    "# # User query\n",
    "# response = chain.invoke(\n",
    "#     {\"input\": \"Identify a paper that deals with language modeling and its scalability.\"}\n",
    "# )\n",
    "\n",
    "# # Get the Answer only\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    openAIclient, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(openAIclient, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper titled \"TextGrad: Automatic Differentiation via Text\" covers the categories of Question Answering and Specificity.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"List the categories covered by the paper titled 'TextGrad: Automatic Differentiation viaText'.\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"dummy\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversational_rag_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconversational_rag_chain\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversational_rag_chain' is not defined"
     ]
    }
   ],
   "source": [
    "conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
